Design for a library in C++ to evaluate an index for the text of a book.
raw = nltk.clean_html(html)
tokens = nltk.word_tokenize(raw)
text.collocations()
text.concordance('gene')
text = nltk.Text(tokens)
words = [w.lower() for w in text)]
vocab = sorted(set(words))
disp = text.dispersion_plot([w for w in vocab])
index = [w for w in disp.density > threshold]
The purpose of this design is to quickly generate an index and not to create another 
natural language processing library. At the same time, use of such library to generate 
NGrams as index candidates is not restricted. We build a table on the vocab as mentioned 
above and with the following attributes: offsets, frequency, Tags, emphasization, PMI
page number, heading, subheading, BiGram Word, Trigram words, collocation words. This 
table can be in an in-memory data store where each of the tuples are stored as nodes in 
a lock-free skip-list. The words will be parsed and populated in our table as they are 
encountered. Page numbers are calculated based on offsets or from document rendering. Whenever 
the rendering changes or the begin and last offset of each page changes, the table will need 
to be updated. The table respects the headings and other markup information available for the 
text that may be helpful to evaluate index candidates such as headings, subheadings, 
bold, italics, quotation marks etc. But with all else equal such as the words in a paragraph 
sharing the same sub-heading and spanning several pages, the criteria to evaluate index candidates
would be based on Pointwise Mutual Information. PMI classification would be done separately as 
available from tag generation. 
In order to find the index-able words, an information extraction system can be built. An Information extraction system searches large bodies of unrestricted text for specific types of entities and relations, and use them to populate well-organized databases. These databases can then be used to find answers for specific questions.
The typical architecture for an information extraction system begins by segmenting, tokenizing, and part-of-speech tagging the text. The resulting data is then searched for specific types of entity. Finally, the information extraction system looks at entities that are mentioned near one another in the text, and tries to determine whether specific relationships hold between those entities.
Entity recognition is often performed using chunkers, which segment multi-token sequences, and label them with the appropriate entity type. Common entity types include who, what, when, where etc. Chunkers can be constructed using rule-based systems, such as the RegexpParser class provided by NLTK; or using machine learning techniques, such as the ConsecutiveNPChunker. 
Although chunkers are specialized to create relatively flat data structures, where no two chunks are allowed to overlap, they can be cascaded together to build nested structures.
Relation extraction can be performed using either rule-based systems which typically look for specific patterns in the text that connect entities and the intervening words; or using machine-learning systems which typically attempt to learn such patterns automatically from a training corpus.
One way that we can incorporate information about the content of words is to use a classifier-based tagger to chunk the sentence. Like the n-gram chunker considered in the previous section, this classifier-based chunker will work by assigning IOB tags to the words in a sentence, and then converting those tags to chunks. 
Chunkers can use different features extractor such as a MaxentClassifier and NaiveBayesClassifier. 

